{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Personal_contribution/DynamicSGD_starting_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sSBZt98ePGy",
        "outputId": "c642e598-1e92-4926-ef47-6e897074b569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localSGD.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localSGD.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "total_computation_time = 0\n",
        "total_communication_time = 0\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "iteration_per_epoch = 782 # number of local step performed in one epoch\n",
        "num_epochs = 150\n",
        "batch_size = 64\n",
        "base_learning_rate = 1e-3\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "#warmup_epochs = 5\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "#global_iteration_per_epoch = math.ceil(global_iteration_per_epoch / (K * J)) * (K * J)\n",
        "#global_iteration_per_epoch = global_iteration_per_epoch // (K * J)\n",
        "#print(f\"Global iteration per epoch: {global_iteration_per_epoch}\")\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "# After creating the DataLoader for each worker, create an iterator\n",
        "shard_iterators = [iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True)) for k in range(K)]\n",
        "\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate: {learning_rate:.5f}\")\n",
        "\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for net in nets[1:]:  # Skip nets[0] because it's the reference model\n",
        "        for param_target, param_source in zip(net.parameters(), nets[0].parameters()):\n",
        "            param_target.data.copy_(param_source.data)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs) for optimizer in optimizers]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize storage for train losses and accuracies for each worker\n",
        "train_losses_per_worker = [[] for _ in range(K)]\n",
        "train_accuracies_per_worker = [[] for _ in range(K)]\n",
        "\n",
        "# Initialize test losses and accuracies for test set\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Compute gradient magnitude and variance\n",
        "def compute_gradient_stats(nets, optimizers):\n",
        "    # Get gradients from each worker\n",
        "    gradients = []\n",
        "    for net, optimizer in zip(nets, optimizers):\n",
        "        grad_vector = []\n",
        "        for param in net.parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_vector.append(param.grad.view(-1))\n",
        "        grad_vector = torch.cat(grad_vector)  # Flatten all gradients into a vector\n",
        "        gradients.append(grad_vector)\n",
        "\n",
        "    # Compute average gradient (g_t)\n",
        "    avg_gradient = torch.stack(gradients).mean(dim=0)\n",
        "\n",
        "    # Compute squared norms of individual gradients\n",
        "    squared_norms = [torch.norm(g).item() ** 2 for g in gradients]\n",
        "\n",
        "    # Compute squared norm of the average gradient\n",
        "    avg_squared_norm = torch.norm(avg_gradient).item() ** 2\n",
        "\n",
        "    # Compute gradient variance (sigma_t^2) using the provided formula\n",
        "    sigma_t_squared = (sum(squared_norms) / (K - 1)) - (K / (K - 1)) * avg_squared_norm\n",
        "\n",
        "    # Compute gradient magnitude (G_t) using the provided formula\n",
        "    G_t = avg_squared_norm - (1 / K) * sigma_t_squared\n",
        "\n",
        "    return G_t, sigma_t_squared\n",
        "\n",
        "\n",
        "# Update H dynamically based on gradient stats\n",
        "def update_H(G_t, sigma_t_squared, H_min, H_max, alpha, epsilon):\n",
        "    H_t = min(H_max, max(H_min, alpha * G_t / (sigma_t_squared + epsilon)))\n",
        "    #H_t = min(H_t, H_max)  # Ensure H_t doesn't exceed H_max\n",
        "    return int(H_t)  # Ensure H_t is an integer\n",
        "\n",
        "# Training loop\n",
        "# Hyperparameters for dynamic adjustment\n",
        "H_min, H_max = 4, 64  # Minimum and maximum allowed local steps\n",
        "alpha, epsilon = 500, 1e-6  # Scaling factor and small constant\n",
        "\n",
        "H_t = J  # Initialize H_t with the default J value\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "    train_loss_for_iteration = [0.0 for _ in range(K)]\n",
        "    correct_train_for_iteration = [0 for _ in range(K)]\n",
        "    total_train_for_iteration = [0 for _ in range(K)]\n",
        "\n",
        "    iteration_per_epoch = 782\n",
        "    iteration = 0 # Number of global iteration performed\n",
        "    while iteration_per_epoch > 0:\n",
        "        computation_start_time = time.time()\n",
        "        # Train each worker for H_t steps\n",
        "        for k in range(K):\n",
        "            nets[k].train()\n",
        "            correct_train, total_train, train_loss = 0, 0, 0.0\n",
        "            for _ in range(H_t):  # Perform H_t local steps\n",
        "                try:\n",
        "                    images, labels = next(shard_iterators[k])\n",
        "                except StopIteration:\n",
        "                    shard_iterators[k] = iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True))\n",
        "                    images, labels = next(shard_iterators[k])\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizers[k].zero_grad()\n",
        "                outputs = nets[k](images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizers[k].step()\n",
        "\n",
        "                # Calculate train loss and accuracy\n",
        "                train_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_loss_for_iteration[k] += train_loss\n",
        "            correct_train_for_iteration[k] += correct_train\n",
        "            total_train_for_iteration[k] += total_train\n",
        "\n",
        "        computation_end_time = time.time()\n",
        "        total_computation_time += (computation_end_time - computation_start_time)\n",
        "\n",
        "        communication_start_time = time.time()\n",
        "        # Synchronize models\n",
        "        with torch.no_grad():\n",
        "            global_parameters = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "            for net in nets:\n",
        "                for global_param, local_param in zip(global_parameters, net.parameters()):\n",
        "                    global_param += local_param\n",
        "            for global_param in global_parameters:\n",
        "                global_param /= K\n",
        "            for net in nets:\n",
        "                for local_param, global_param in zip(net.parameters(), global_parameters):\n",
        "                    local_param.data.copy_(global_param)\n",
        "\n",
        "        communication_end_time = time.time()\n",
        "        total_communication_time += (communication_end_time - communication_start_time)\n",
        "\n",
        "        # update tot_num_local_step and number of global iteration\n",
        "        iteration_per_epoch -= (H_t * K)\n",
        "        iteration += 1\n",
        "        # Compute gradient stats and update H_t\n",
        "        G_t, sigma_t_squared = compute_gradient_stats(nets, optimizers)\n",
        "        H_t = update_H(G_t, sigma_t_squared, H_min, H_max, alpha, epsilon)\n",
        "        print(f\"Epoch {epoch+1}, Iteration {iteration}: H_t = {H_t}, G_t = {G_t:.4f}, σ_t^2 = {sigma_t_squared:.4f}\")\n",
        "\n",
        "    # Store training metrics for all workers after each epoch\n",
        "    for k in range(K):\n",
        "        # Store train loss and accuracy for this worker\n",
        "        train_loss_for_iteration[k] /= total_train_for_iteration[k]\n",
        "        train_losses_per_worker[k].append(train_loss_for_iteration[k])\n",
        "        tot_accuracy = 100. * correct_train_for_iteration[k] / total_train_for_iteration[k]\n",
        "        train_accuracies_per_worker[k].append(tot_accuracy)\n",
        "        print(f\"    Worker {k}: Train Loss: {train_losses_per_worker[k][-1]:.4f}, Train Accuracy: {train_accuracies_per_worker[k][-1]:.2f}%\")\n",
        "\n",
        "    # Update learning rate\n",
        "    for scheduler in schedulers:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    global_model = nets[0]\n",
        "    global_model.eval()\n",
        "    correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = 100. * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    print(f\"Epoch {epoch + 1}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(f\"Total Computation Time: {total_computation_time:.2f} seconds\")\n",
        "    print(f\"Total Communication Time: {total_communication_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot Train Loss for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_losses_per_worker[k], label=f'Worker {k} Train Loss')\n",
        "plt.title('Train Loss per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracy for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_accuracies_per_worker[k], label=f'Worker {k} Train Accuracy')\n",
        "plt.title('Train Accuracy per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MMfqfbnNFSn3",
        "outputId": "d07027af-bcf8-4ec6-9431-880bdd1596cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with K=4, J=32\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:11<00:00, 14.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Base learning rate: 0.00400\n",
            "Epoch 1, Iteration 1: H_t = 24, G_t = 0.0062, σ_t^2 = 0.1271\n",
            "Epoch 1, Iteration 2: H_t = 20, G_t = 0.0049, σ_t^2 = 0.1193\n",
            "Epoch 1, Iteration 3: H_t = 6, G_t = 0.0018, σ_t^2 = 0.1401\n",
            "Epoch 1, Iteration 4: H_t = 29, G_t = 0.0067, σ_t^2 = 0.1156\n",
            "Epoch 1, Iteration 5: H_t = 59, G_t = 0.0165, σ_t^2 = 0.1371\n",
            "Epoch 1, Iteration 6: H_t = 14, G_t = 0.0069, σ_t^2 = 0.2455\n",
            "Epoch 1, Iteration 7: H_t = 11, G_t = 0.0068, σ_t^2 = 0.3032\n",
            "Epoch 1, Iteration 8: H_t = 4, G_t = -0.0034, σ_t^2 = 0.3681\n",
            "Epoch 1, Iteration 9: H_t = 15, G_t = 0.0115, σ_t^2 = 0.3759\n",
            "    Worker 0: Train Loss: 4.5993, Train Accuracy: 1.39%\n",
            "    Worker 1: Train Loss: 4.6001, Train Accuracy: 1.29%\n",
            "    Worker 2: Train Loss: 4.5983, Train Accuracy: 1.64%\n",
            "    Worker 3: Train Loss: 4.5992, Train Accuracy: 1.48%\n",
            "Epoch 1: Test Loss: 4.5818, Test Accuracy: 2.66%\n",
            "Epoch 2, Iteration 1: H_t = 56, G_t = 0.0564, σ_t^2 = 0.4954\n",
            "Epoch 2, Iteration 2: H_t = 8, G_t = 0.0195, σ_t^2 = 1.1075\n",
            "Epoch 2, Iteration 3: H_t = 4, G_t = -0.0151, σ_t^2 = 1.3983\n",
            "Epoch 2, Iteration 4: H_t = 13, G_t = 0.0440, σ_t^2 = 1.6027\n",
            "Epoch 2, Iteration 5: H_t = 4, G_t = 0.0008, σ_t^2 = 2.1657\n",
            "Epoch 2, Iteration 6: H_t = 24, G_t = 0.1028, σ_t^2 = 2.1126\n",
            "Epoch 2, Iteration 7: H_t = 20, G_t = 0.1508, σ_t^2 = 3.7262\n",
            "Epoch 2, Iteration 8: H_t = 4, G_t = -0.1049, σ_t^2 = 4.6742\n",
            "Epoch 2, Iteration 9: H_t = 9, G_t = 0.0922, σ_t^2 = 4.7558\n",
            "Epoch 2, Iteration 10: H_t = 4, G_t = -0.0184, σ_t^2 = 5.2653\n",
            "Epoch 2, Iteration 11: H_t = 64, G_t = 0.8158, σ_t^2 = 5.6982\n",
            "Epoch 2, Iteration 12: H_t = 7, G_t = 0.1074, σ_t^2 = 7.1898\n",
            "    Worker 0: Train Loss: 4.4607, Train Accuracy: 3.32%\n",
            "    Worker 1: Train Loss: 4.4577, Train Accuracy: 3.36%\n",
            "    Worker 2: Train Loss: 4.4481, Train Accuracy: 3.46%\n",
            "    Worker 3: Train Loss: 4.4508, Train Accuracy: 3.48%\n",
            "Epoch 2: Test Loss: 4.1962, Test Accuracy: 6.04%\n",
            "Epoch 3, Iteration 1: H_t = 4, G_t = 0.0795, σ_t^2 = 8.5208\n",
            "Epoch 3, Iteration 2: H_t = 4, G_t = 0.0422, σ_t^2 = 8.3169\n",
            "Epoch 3, Iteration 3: H_t = 14, G_t = 0.2858, σ_t^2 = 9.6168\n",
            "Epoch 3, Iteration 4: H_t = 5, G_t = 0.1714, σ_t^2 = 14.2960\n",
            "Epoch 3, Iteration 5: H_t = 23, G_t = 0.6236, σ_t^2 = 13.0282\n",
            "Epoch 3, Iteration 6: H_t = 4, G_t = -0.1703, σ_t^2 = 11.7529\n",
            "Epoch 3, Iteration 7: H_t = 4, G_t = 0.0037, σ_t^2 = 10.2001\n",
            "Epoch 3, Iteration 8: H_t = 27, G_t = 0.5870, σ_t^2 = 10.7488\n",
            "Epoch 3, Iteration 9: H_t = 4, G_t = 0.0454, σ_t^2 = 12.1781\n",
            "Epoch 3, Iteration 10: H_t = 12, G_t = 0.2864, σ_t^2 = 11.4313\n",
            "Epoch 3, Iteration 11: H_t = 4, G_t = -0.1329, σ_t^2 = 14.2803\n",
            "Epoch 3, Iteration 12: H_t = 4, G_t = -0.1989, σ_t^2 = 13.4315\n",
            "Epoch 3, Iteration 13: H_t = 11, G_t = 0.2931, σ_t^2 = 12.6380\n",
            "Epoch 3, Iteration 14: H_t = 4, G_t = -0.2054, σ_t^2 = 14.6548\n",
            "Epoch 3, Iteration 15: H_t = 12, G_t = 0.3043, σ_t^2 = 12.5585\n",
            "Epoch 3, Iteration 16: H_t = 4, G_t = -0.0164, σ_t^2 = 16.7520\n",
            "Epoch 3, Iteration 17: H_t = 23, G_t = 0.6278, σ_t^2 = 13.4097\n",
            "Epoch 3, Iteration 18: H_t = 23, G_t = 0.5889, σ_t^2 = 12.5903\n",
            "Epoch 3, Iteration 19: H_t = 4, G_t = -0.5368, σ_t^2 = 11.4180\n",
            "Epoch 3, Iteration 20: H_t = 18, G_t = 0.3756, σ_t^2 = 10.3782\n",
            "    Worker 0: Train Loss: 4.1168, Train Accuracy: 6.95%\n",
            "    Worker 1: Train Loss: 4.1265, Train Accuracy: 6.84%\n",
            "    Worker 2: Train Loss: 4.0872, Train Accuracy: 7.04%\n",
            "    Worker 3: Train Loss: 4.1290, Train Accuracy: 6.57%\n",
            "Epoch 3: Test Loss: 3.9705, Test Accuracy: 9.24%\n",
            "Epoch 4, Iteration 1: H_t = 6, G_t = 0.1395, σ_t^2 = 10.9212\n",
            "Epoch 4, Iteration 2: H_t = 16, G_t = 0.3825, σ_t^2 = 11.4764\n",
            "Epoch 4, Iteration 3: H_t = 28, G_t = 0.8168, σ_t^2 = 14.3572\n",
            "Epoch 4, Iteration 4: H_t = 4, G_t = 0.0282, σ_t^2 = 10.7330\n",
            "Epoch 4, Iteration 5: H_t = 4, G_t = -0.5561, σ_t^2 = 10.5239\n",
            "Epoch 4, Iteration 6: H_t = 34, G_t = 0.7333, σ_t^2 = 10.7606\n",
            "Epoch 4, Iteration 7: H_t = 4, G_t = -0.3845, σ_t^2 = 16.3905\n",
            "Epoch 4, Iteration 8: H_t = 23, G_t = 0.4938, σ_t^2 = 10.2966\n",
            "Epoch 4, Iteration 9: H_t = 4, G_t = 0.0391, σ_t^2 = 13.8539\n",
            "Epoch 4, Iteration 10: H_t = 24, G_t = 0.6969, σ_t^2 = 14.5164\n",
            "Epoch 4, Iteration 11: H_t = 4, G_t = -0.5260, σ_t^2 = 12.4498\n",
            "Epoch 4, Iteration 12: H_t = 64, G_t = 2.6815, σ_t^2 = 11.9534\n",
            "Epoch 4, Iteration 13: H_t = 42, G_t = 1.0414, σ_t^2 = 12.3457\n",
            "    Worker 0: Train Loss: 3.9810, Train Accuracy: 8.66%\n",
            "    Worker 1: Train Loss: 3.9804, Train Accuracy: 8.71%\n",
            "    Worker 2: Train Loss: 3.9350, Train Accuracy: 9.27%\n",
            "    Worker 3: Train Loss: 3.9744, Train Accuracy: 9.14%\n",
            "Epoch 4: Test Loss: 3.8086, Test Accuracy: 11.89%\n",
            "Epoch 5, Iteration 1: H_t = 60, G_t = 1.7568, σ_t^2 = 14.4041\n",
            "Epoch 5, Iteration 2: H_t = 36, G_t = 0.8438, σ_t^2 = 11.5034\n",
            "Epoch 5, Iteration 3: H_t = 4, G_t = -0.4139, σ_t^2 = 14.2491\n",
            "Epoch 5, Iteration 4: H_t = 15, G_t = 0.3774, σ_t^2 = 12.3553\n",
            "Epoch 5, Iteration 5: H_t = 4, G_t = -0.2900, σ_t^2 = 12.9841\n",
            "Epoch 5, Iteration 6: H_t = 4, G_t = 0.0601, σ_t^2 = 15.2550\n",
            "Epoch 5, Iteration 7: H_t = 4, G_t = -0.1491, σ_t^2 = 14.8557\n",
            "Epoch 5, Iteration 8: H_t = 63, G_t = 1.8511, σ_t^2 = 14.4668\n",
            "Epoch 5, Iteration 9: H_t = 4, G_t = -0.1594, σ_t^2 = 11.9056\n",
            "    Worker 0: Train Loss: 3.8426, Train Accuracy: 10.75%\n",
            "    Worker 1: Train Loss: 3.8561, Train Accuracy: 10.83%\n",
            "    Worker 2: Train Loss: 3.7999, Train Accuracy: 11.60%\n",
            "    Worker 3: Train Loss: 3.8523, Train Accuracy: 10.91%\n",
            "Epoch 5: Test Loss: 3.6764, Test Accuracy: 14.45%\n",
            "Epoch 6, Iteration 1: H_t = 29, G_t = 0.7592, σ_t^2 = 12.8572\n",
            "Epoch 6, Iteration 2: H_t = 4, G_t = -0.6166, σ_t^2 = 20.9080\n",
            "Epoch 6, Iteration 3: H_t = 64, G_t = 2.0060, σ_t^2 = 11.6833\n",
            "Epoch 6, Iteration 4: H_t = 4, G_t = -0.0466, σ_t^2 = 13.7075\n",
            "Epoch 6, Iteration 5: H_t = 17, G_t = 0.4823, σ_t^2 = 14.0455\n",
            "Epoch 6, Iteration 6: H_t = 4, G_t = -0.7211, σ_t^2 = 18.4593\n",
            "Epoch 6, Iteration 7: H_t = 4, G_t = -0.4650, σ_t^2 = 15.5239\n",
            "Epoch 6, Iteration 8: H_t = 7, G_t = 0.2561, σ_t^2 = 17.7045\n",
            "Epoch 6, Iteration 9: H_t = 46, G_t = 1.3702, σ_t^2 = 14.5777\n",
            "Epoch 6, Iteration 10: H_t = 4, G_t = -0.4435, σ_t^2 = 16.0858\n",
            "Epoch 6, Iteration 11: H_t = 4, G_t = -0.3888, σ_t^2 = 18.8639\n",
            "Epoch 6, Iteration 12: H_t = 4, G_t = -0.1868, σ_t^2 = 15.5825\n",
            "Epoch 6, Iteration 13: H_t = 21, G_t = 0.6637, σ_t^2 = 15.4342\n",
            "Epoch 6, Iteration 14: H_t = 4, G_t = -0.1044, σ_t^2 = 18.8671\n",
            "    Worker 0: Train Loss: 3.7290, Train Accuracy: 12.67%\n",
            "    Worker 1: Train Loss: 3.7149, Train Accuracy: 13.15%\n",
            "    Worker 2: Train Loss: 3.6872, Train Accuracy: 13.36%\n",
            "    Worker 3: Train Loss: 3.7152, Train Accuracy: 13.22%\n",
            "Epoch 6: Test Loss: 3.5803, Test Accuracy: 15.91%\n",
            "Epoch 7, Iteration 1: H_t = 10, G_t = 0.4123, σ_t^2 = 19.2561\n",
            "Epoch 7, Iteration 2: H_t = 34, G_t = 1.2932, σ_t^2 = 18.7538\n",
            "Epoch 7, Iteration 3: H_t = 4, G_t = -0.3650, σ_t^2 = 15.2442\n",
            "Epoch 7, Iteration 4: H_t = 14, G_t = 0.4779, σ_t^2 = 16.1476\n",
            "Epoch 7, Iteration 5: H_t = 8, G_t = 0.2372, σ_t^2 = 14.6167\n",
            "Epoch 7, Iteration 6: H_t = 20, G_t = 0.8324, σ_t^2 = 20.5588\n",
            "Epoch 7, Iteration 7: H_t = 4, G_t = 0.0657, σ_t^2 = 18.7180\n",
            "Epoch 7, Iteration 8: H_t = 4, G_t = -0.6359, σ_t^2 = 19.1018\n",
            "Epoch 7, Iteration 9: H_t = 4, G_t = -0.5858, σ_t^2 = 17.8940\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/localSGD.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# Calculate train loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mtotal_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%run localSGD.py --k 4 --j 32"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}