{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Distributed_LocalSGD_command_line.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqBQ7RmFZ_X",
        "outputId": "749e84a4-dd0c-4c24-997b-c1dd1f23e7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localSGD.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localSGD.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "num_epochs = 150\n",
        "base_learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "warmup_epochs = 5\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    \"\"\"Compute the mean and std of CIFAR-100 dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: A dataset derived from `torch.utils.data.Dataset`,\n",
        "                 such as `cifar100_training_dataset` or `cifar100_test_dataset`.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing (mean, std) for the entire dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "class LRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, base_lr, K, warmup_epochs, total_epochs, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            optimizer: SGD\n",
        "            base_lr: Base learning rate for the reference batch size.\n",
        "            K: Number of workers.\n",
        "            warmup_epochs: Number of warmup epochs.\n",
        "            total_epochs: Total number of epochs.\n",
        "            verbose: Whether to print LR updates.\n",
        "        \"\"\"\n",
        "        self.base_lr = base_lr\n",
        "        self.K = K\n",
        "        self.scaled_lr = base_lr * K\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.verbose = verbose\n",
        "        super(LRScheduler, self).__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        epoch = self.last_epoch\n",
        "        if epoch < self.warmup_epochs:\n",
        "            # Linear warmup scaling\n",
        "            warmup_factor = (epoch + 1) / self.warmup_epochs\n",
        "            return [warmup_factor * self.scaled_lr for _ in self.optimizer.param_groups]\n",
        "        else:\n",
        "            # polynomially decaying learning rate of ηt = η0×(1−t/T)\n",
        "            if epoch == self.warmup_epochs:\n",
        "                print(f\"Warmup phase completed at epoch {epoch}. Switching to polynomial decay learning rate.\")\n",
        "            return [self.scaled_lr * ( 1 - ( (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs))) for _ in self.optimizer.param_groups]\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate: {learning_rate:.5f}\")\n",
        "\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "with torch.no_grad():\n",
        "    global_parameters = [torch.zeros_like(param) for param in nets[0].parameters()]  # Start with the first worker\n",
        "    for net in nets:\n",
        "        for local_param, global_param in zip(net.parameters(), global_parameters):\n",
        "            local_param.data.copy_(global_param)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [LRScheduler(\n",
        "    optimizer = optimizer,\n",
        "    base_lr = base_learning_rate,\n",
        "    K = K,\n",
        "    warmup_epochs = warmup_epochs,\n",
        "    total_epochs = num_epochs,\n",
        "    verbose = True) for optimizer in optimizers]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Sequentially train each worker to simulate a distributed environment\n",
        "    for k in range(K):\n",
        "        batch_size = int(len(shards[k]) / J)\n",
        "        shard_loader = DataLoader(shards[k], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        nets[k].train()\n",
        "        for images, labels in shard_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "            optimizers[k].zero_grad()\n",
        "            outputs = nets[k](images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizers[k].step()\n",
        "\n",
        "    # Perform model synchronization\n",
        "    with torch.no_grad():\n",
        "        global_parameters = [torch.zeros_like(param) for param in nets[0].parameters()]  # Start with the first worker\n",
        "        for net in nets:\n",
        "            for global_param, local_param in zip(global_parameters, net.parameters()):\n",
        "                global_param += local_param\n",
        "        for global_param in global_parameters:\n",
        "            global_param /= K\n",
        "        # Update each worker model with the global parameters\n",
        "        for net in nets:\n",
        "            for local_param, global_param in zip(net.parameters(), global_parameters):\n",
        "                local_param.data.copy_(global_param)\n",
        "\n",
        "    # Update lr for each scheduler\n",
        "    for scheduler in schedulers:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Final Evaluation on Test Set\n",
        "    global_model = nets[0] # all models are synchronized\n",
        "    global_model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "            outputs = global_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy for K={K}, J={J}: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW9c_PRbwGQw",
        "outputId": "22cb1290-4750-4eec-ab46-3ff547866e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with K=2, J=4\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "%run localSGD.py --k 2 --j 4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}