{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Distributed_LocalSGD_SlowMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqBQ7RmFZ_X",
        "outputId": "7131eef2-8f53-4042-9561-e3f790a008c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localSGD.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localSGD.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "# Alpha slow learning rate a=1 best\n",
        "# Beta slow momentum\n",
        "global_iteration_per_epoch = 782 # to be divided by K*J\n",
        "num_epochs = 150\n",
        "batch_size = 64\n",
        "base_learning_rate = 1e-3\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "warmup_epochs = 5\n",
        "\n",
        "alpha = 1.0  # Slow learning rate scaling factor\n",
        "beta = 0  # Slow momentum factor\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    \"\"\"Compute the mean and std of CIFAR-100 dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: A dataset derived from `torch.utils.data.Dataset`,\n",
        "                 such as `cifar100_training_dataset` or `cifar100_test_dataset`.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing (mean, std) for the entire dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "global_iteration_per_epoch = math.ceil(global_iteration_per_epoch / (K * J)) * (K * J)\n",
        "global_iteration_per_epoch = global_iteration_per_epoch // (K * J)\n",
        "print(f\"Global iteration per epoch: {global_iteration_per_epoch}\")\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "# After creating the DataLoader for each worker, create an iterator\n",
        "shard_iterators = [iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True)) for k in range(K)]\n",
        "\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate for each k: {learning_rate:.5f}\")\n",
        "lr_warmup=learning_rate\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for net in nets[1:]:  # Skip nets[0] because it's the reference model\n",
        "        for param_target, param_source in zip(net.parameters(), nets[0].parameters()):\n",
        "            param_target.data.copy_(param_source.data)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs - warmup_epochs) for optimizer in optimizers]\n",
        "\n",
        "u_buffer = [torch.zeros_like(param) for param in nets[0].parameters()]  # Momentum buffer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize storage for train losses and accuracies for each worker\n",
        "train_losses_per_worker = [[] for _ in range(K)]\n",
        "train_accuracies_per_worker = [[] for _ in range(K)]\n",
        "\n",
        "# Initialize test losses and accuracies for test set\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "    if epoch < warmup_epochs: #lr for buffer update during warmup\n",
        "            w_factor = (epoch + 1) / warmup_epochs\n",
        "            lr_warmup = w_factor * learning_rate\n",
        "    train_loss_for_iteration = [0.0 for _ in range(K)]\n",
        "    correct_train_for_iteration = [0 for _ in range(K)]\n",
        "    total_train_for_iteration = [0 for _ in range(K)]\n",
        "    for iteration in range(global_iteration_per_epoch): #outer loop\n",
        "\n",
        "        #x(t,0)\n",
        "        iter_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "        for param, g_param in zip(iter_params, nets[0].parameters()):\n",
        "                    param.data.copy_(g_param)\n",
        "        # Sequentially train each worker to simulate a distributed environment\n",
        "        for k in range(K):\n",
        "\n",
        "            nets[k].train()\n",
        "            correct_train, total_train, train_loss = 0, 0, 0.0\n",
        "\n",
        "            # Perform J local step before synchronization\n",
        "            for j in range(J): #inner loop\n",
        "                      try:\n",
        "                          # Get a batch from the current worker's shard using next()\n",
        "                          images, labels = next(shard_iterators[k])\n",
        "                          images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "                      except StopIteration:\n",
        "                          # If the iterator has exhausted the data, restart the iterator\n",
        "                          shard_iterators[k] = iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True))\n",
        "                          images, labels = next(shard_iterators[k]) # Resume from the next batch\n",
        "                          images, labels = images.to(device), labels.to(device)\n",
        "                          # Perform optimization step\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_loss_for_iteration[k] += train_loss\n",
        "            correct_train_for_iteration[k] += correct_train\n",
        "            total_train_for_iteration[k] += total_train\n",
        "\n",
        "\n",
        "         # Synchronization (SLOWMO Algothm 1 - paper 21)\n",
        "        with torch.no_grad():\n",
        "            global_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "            #x(t, tau)\n",
        "            for net in nets:\n",
        "                for global_param, local_param in zip(global_params, net.parameters()):\n",
        "                    global_param += local_param\n",
        "            for global_param in global_params:\n",
        "                global_param /= K\n",
        "\n",
        "            for param, g_param, u_param in zip(iter_params, global_params, u_buffer):#sync with warmup\n",
        "                if epoch < warmup_epochs:\n",
        "                    u_param.mul_(beta).add_((param - g_param) / lr_warmup)\n",
        "                    g_param.sub_(alpha * lr_warmup * u_param)\n",
        "\n",
        "                else: #Sync after warmup\n",
        "                    #print(f\"{optimizers[0].param_groups[0]['lr']}  {optimizers[1].param_groups[0]['lr']}\")\n",
        "                    lr_sched = optimizers[0].param_groups[0]['lr'] #optimizer lr\n",
        "                    u_param.mul_(beta).add_((param - g_param) / lr_sched)\n",
        "                    g_param.sub_(alpha * lr_sched * u_param)\n",
        "\n",
        "            for net in nets:\n",
        "                for param, g_param in zip(net.parameters(), global_params):\n",
        "                    param.data.copy_(g_param)\n",
        "\n",
        "    # Update lr for each scheduler\n",
        "    for scheduler in schedulers:\n",
        "        if epoch >= warmup_epochs:\n",
        "            scheduler.step()  # Trigger cosine decay\n",
        "        else:\n",
        "            warmup_factor = (epoch + 1) / warmup_epochs\n",
        "            lr = warmup_factor * learning_rate\n",
        "            for optimizer in optimizers:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "        # print(f\"Learning rate at epoch {epoch + 1}: {scheduler.get_lr()[0]:.5f}\")\n",
        "\n",
        "    # Final Evaluation on Test Set\n",
        "    global_model = nets[0] # all models are synchronized\n",
        "    global_model.eval()\n",
        "    # Test\n",
        "    correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = 100. * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    for k in range(K):\n",
        "        # Store train loss and accuracy for this worker\n",
        "        train_loss_for_iteration[k] /= total_train_for_iteration[k]\n",
        "        train_losses_per_worker[k].append(train_loss_for_iteration[k])\n",
        "        tot_accuracy = 100. * correct_train_for_iteration[k] / total_train_for_iteration[k]\n",
        "        train_accuracies_per_worker[k].append(tot_accuracy)\n",
        "        print(f\"    Worker {k}: Train Loss: {train_losses_per_worker[k][-1]:.4f}, Train Accuracy: {train_accuracies_per_worker[k][-1]:.2f}%\")\n",
        "\n",
        "# Plot results\n",
        "# Plot Train Loss for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_losses_per_worker[k], label=f'Worker {k} Train Loss')\n",
        "plt.title('Train Loss per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracy for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_accuracies_per_worker[k], label=f'Worker {k} Train Accuracy')\n",
        "plt.title('Train Accuracy per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW9c_PRbwGQw",
        "outputId": "3b3e034b-647e-48f0-8a80-6bb6e2f64231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with K=8, J=4\n",
            "Global iteration per epoch: 25\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Base learning rate for each k: 0.00800\n",
            "Epoch 1/150: Test Loss: 4.3678, Test Accuracy: 4.25%\n",
            "    Worker 0: Train Loss: 4.5630, Train Accuracy: 2.10%\n",
            "    Worker 1: Train Loss: 4.5686, Train Accuracy: 2.04%\n",
            "    Worker 2: Train Loss: 4.5668, Train Accuracy: 2.10%\n",
            "    Worker 3: Train Loss: 4.5645, Train Accuracy: 2.67%\n",
            "    Worker 4: Train Loss: 4.5632, Train Accuracy: 2.10%\n",
            "    Worker 5: Train Loss: 4.5561, Train Accuracy: 2.15%\n",
            "    Worker 6: Train Loss: 4.5635, Train Accuracy: 1.99%\n",
            "    Worker 7: Train Loss: 4.5632, Train Accuracy: 2.13%\n",
            "Epoch 2/150: Test Loss: 4.1504, Test Accuracy: 6.58%\n",
            "    Worker 0: Train Loss: 4.2631, Train Accuracy: 5.41%\n",
            "    Worker 1: Train Loss: 4.2609, Train Accuracy: 5.46%\n",
            "    Worker 2: Train Loss: 4.2636, Train Accuracy: 5.72%\n",
            "    Worker 3: Train Loss: 4.2720, Train Accuracy: 5.36%\n",
            "    Worker 4: Train Loss: 4.2511, Train Accuracy: 5.02%\n",
            "    Worker 5: Train Loss: 4.2408, Train Accuracy: 5.66%\n",
            "    Worker 6: Train Loss: 4.2729, Train Accuracy: 4.99%\n",
            "    Worker 7: Train Loss: 4.2579, Train Accuracy: 5.39%\n",
            "Epoch 3/150: Test Loss: 4.1032, Test Accuracy: 7.28%\n",
            "    Worker 0: Train Loss: 4.1406, Train Accuracy: 6.59%\n",
            "    Worker 1: Train Loss: 4.1540, Train Accuracy: 6.10%\n",
            "    Worker 2: Train Loss: 4.1484, Train Accuracy: 6.65%\n",
            "    Worker 3: Train Loss: 4.1455, Train Accuracy: 6.48%\n",
            "    Worker 4: Train Loss: 4.1274, Train Accuracy: 6.65%\n",
            "    Worker 5: Train Loss: 4.1260, Train Accuracy: 6.46%\n",
            "    Worker 6: Train Loss: 4.1690, Train Accuracy: 6.29%\n",
            "    Worker 7: Train Loss: 4.1664, Train Accuracy: 6.41%\n",
            "Epoch 4/150: Test Loss: 3.9266, Test Accuracy: 9.79%\n",
            "    Worker 0: Train Loss: 4.0371, Train Accuracy: 7.86%\n",
            "    Worker 1: Train Loss: 4.0558, Train Accuracy: 7.49%\n",
            "    Worker 2: Train Loss: 4.0381, Train Accuracy: 8.26%\n",
            "    Worker 3: Train Loss: 4.0543, Train Accuracy: 7.79%\n",
            "    Worker 4: Train Loss: 4.0110, Train Accuracy: 8.18%\n",
            "    Worker 5: Train Loss: 4.0085, Train Accuracy: 8.01%\n",
            "    Worker 6: Train Loss: 4.0521, Train Accuracy: 7.38%\n",
            "    Worker 7: Train Loss: 4.0302, Train Accuracy: 7.89%\n",
            "Epoch 5/150: Test Loss: 3.8412, Test Accuracy: 11.01%\n",
            "    Worker 0: Train Loss: 3.9217, Train Accuracy: 9.34%\n",
            "    Worker 1: Train Loss: 3.9112, Train Accuracy: 9.75%\n",
            "    Worker 2: Train Loss: 3.9124, Train Accuracy: 9.85%\n",
            "    Worker 3: Train Loss: 3.9477, Train Accuracy: 9.14%\n",
            "    Worker 4: Train Loss: 3.8779, Train Accuracy: 10.30%\n",
            "    Worker 5: Train Loss: 3.8941, Train Accuracy: 10.68%\n",
            "    Worker 6: Train Loss: 3.9186, Train Accuracy: 9.25%\n",
            "    Worker 7: Train Loss: 3.9152, Train Accuracy: 9.50%\n",
            "Epoch 6/150: Test Loss: 3.6376, Test Accuracy: 14.71%\n",
            "    Worker 0: Train Loss: 3.8502, Train Accuracy: 10.65%\n",
            "    Worker 1: Train Loss: 3.8416, Train Accuracy: 10.16%\n",
            "    Worker 2: Train Loss: 3.8193, Train Accuracy: 11.37%\n",
            "    Worker 3: Train Loss: 3.8437, Train Accuracy: 10.88%\n",
            "    Worker 4: Train Loss: 3.7903, Train Accuracy: 11.57%\n",
            "    Worker 5: Train Loss: 3.7795, Train Accuracy: 12.04%\n",
            "    Worker 6: Train Loss: 3.8573, Train Accuracy: 10.71%\n",
            "    Worker 7: Train Loss: 3.8263, Train Accuracy: 10.68%\n",
            "Epoch 7/150: Test Loss: 3.5285, Test Accuracy: 16.56%\n",
            "    Worker 0: Train Loss: 3.6525, Train Accuracy: 13.50%\n",
            "    Worker 1: Train Loss: 3.6490, Train Accuracy: 13.72%\n",
            "    Worker 2: Train Loss: 3.6657, Train Accuracy: 13.64%\n",
            "    Worker 3: Train Loss: 3.7047, Train Accuracy: 13.04%\n",
            "    Worker 4: Train Loss: 3.6302, Train Accuracy: 13.86%\n",
            "    Worker 5: Train Loss: 3.6285, Train Accuracy: 14.24%\n",
            "    Worker 6: Train Loss: 3.6784, Train Accuracy: 13.30%\n",
            "    Worker 7: Train Loss: 3.6710, Train Accuracy: 13.66%\n",
            "Epoch 8/150: Test Loss: 3.4716, Test Accuracy: 17.20%\n",
            "    Worker 0: Train Loss: 3.5605, Train Accuracy: 15.29%\n",
            "    Worker 1: Train Loss: 3.5465, Train Accuracy: 15.29%\n",
            "    Worker 2: Train Loss: 3.5544, Train Accuracy: 15.55%\n",
            "    Worker 3: Train Loss: 3.5897, Train Accuracy: 14.66%\n",
            "    Worker 4: Train Loss: 3.5430, Train Accuracy: 15.43%\n",
            "    Worker 5: Train Loss: 3.5188, Train Accuracy: 15.87%\n",
            "    Worker 6: Train Loss: 3.5677, Train Accuracy: 15.32%\n",
            "    Worker 7: Train Loss: 3.5689, Train Accuracy: 15.18%\n",
            "Epoch 9/150: Test Loss: 3.3379, Test Accuracy: 20.40%\n",
            "    Worker 0: Train Loss: 3.4925, Train Accuracy: 16.75%\n",
            "    Worker 1: Train Loss: 3.4842, Train Accuracy: 16.65%\n",
            "    Worker 2: Train Loss: 3.4830, Train Accuracy: 16.85%\n",
            "    Worker 3: Train Loss: 3.4841, Train Accuracy: 16.53%\n",
            "    Worker 4: Train Loss: 3.4533, Train Accuracy: 17.51%\n",
            "    Worker 5: Train Loss: 3.4356, Train Accuracy: 17.80%\n",
            "    Worker 6: Train Loss: 3.4886, Train Accuracy: 16.49%\n",
            "    Worker 7: Train Loss: 3.4803, Train Accuracy: 16.07%\n",
            "Epoch 10/150: Test Loss: 3.3515, Test Accuracy: 19.90%\n",
            "    Worker 0: Train Loss: 3.3443, Train Accuracy: 18.99%\n",
            "    Worker 1: Train Loss: 3.3462, Train Accuracy: 18.70%\n",
            "    Worker 2: Train Loss: 3.3649, Train Accuracy: 19.07%\n",
            "    Worker 3: Train Loss: 3.4012, Train Accuracy: 18.14%\n",
            "    Worker 4: Train Loss: 3.3102, Train Accuracy: 19.93%\n",
            "    Worker 5: Train Loss: 3.3127, Train Accuracy: 19.76%\n",
            "    Worker 6: Train Loss: 3.3804, Train Accuracy: 18.53%\n",
            "    Worker 7: Train Loss: 3.3387, Train Accuracy: 19.94%\n",
            "Epoch 11/150: Test Loss: 3.1745, Test Accuracy: 23.11%\n",
            "    Worker 0: Train Loss: 3.3231, Train Accuracy: 19.76%\n",
            "    Worker 1: Train Loss: 3.2660, Train Accuracy: 20.45%\n",
            "    Worker 2: Train Loss: 3.3305, Train Accuracy: 20.49%\n",
            "    Worker 3: Train Loss: 3.3169, Train Accuracy: 19.50%\n",
            "    Worker 4: Train Loss: 3.2689, Train Accuracy: 20.46%\n",
            "    Worker 5: Train Loss: 3.2548, Train Accuracy: 20.43%\n",
            "    Worker 6: Train Loss: 3.3122, Train Accuracy: 19.35%\n",
            "    Worker 7: Train Loss: 3.3105, Train Accuracy: 19.25%\n",
            "Epoch 12/150: Test Loss: 3.0664, Test Accuracy: 24.93%\n",
            "    Worker 0: Train Loss: 3.1869, Train Accuracy: 21.93%\n",
            "    Worker 1: Train Loss: 3.1805, Train Accuracy: 22.12%\n",
            "    Worker 2: Train Loss: 3.1777, Train Accuracy: 22.70%\n",
            "    Worker 3: Train Loss: 3.2040, Train Accuracy: 21.09%\n",
            "    Worker 4: Train Loss: 3.1732, Train Accuracy: 21.79%\n",
            "    Worker 5: Train Loss: 3.1588, Train Accuracy: 22.48%\n",
            "    Worker 6: Train Loss: 3.2087, Train Accuracy: 22.25%\n",
            "    Worker 7: Train Loss: 3.1741, Train Accuracy: 21.75%\n",
            "Epoch 13/150: Test Loss: 3.0263, Test Accuracy: 25.54%\n",
            "    Worker 0: Train Loss: 3.1279, Train Accuracy: 22.78%\n",
            "    Worker 1: Train Loss: 3.1025, Train Accuracy: 23.46%\n",
            "    Worker 2: Train Loss: 3.1625, Train Accuracy: 23.17%\n",
            "    Worker 3: Train Loss: 3.1857, Train Accuracy: 21.75%\n",
            "    Worker 4: Train Loss: 3.1172, Train Accuracy: 23.27%\n",
            "    Worker 5: Train Loss: 3.1172, Train Accuracy: 23.85%\n",
            "    Worker 6: Train Loss: 3.1613, Train Accuracy: 22.99%\n",
            "    Worker 7: Train Loss: 3.1632, Train Accuracy: 22.37%\n",
            "Epoch 14/150: Test Loss: 2.9872, Test Accuracy: 26.33%\n",
            "    Worker 0: Train Loss: 3.1166, Train Accuracy: 22.75%\n",
            "    Worker 1: Train Loss: 3.1025, Train Accuracy: 23.60%\n",
            "    Worker 2: Train Loss: 3.0791, Train Accuracy: 24.44%\n",
            "    Worker 3: Train Loss: 3.1072, Train Accuracy: 23.80%\n",
            "    Worker 4: Train Loss: 3.0778, Train Accuracy: 24.00%\n",
            "    Worker 5: Train Loss: 3.0802, Train Accuracy: 24.95%\n",
            "    Worker 6: Train Loss: 3.1252, Train Accuracy: 23.80%\n",
            "    Worker 7: Train Loss: 3.0713, Train Accuracy: 23.82%\n",
            "Epoch 15/150: Test Loss: 2.9586, Test Accuracy: 27.37%\n",
            "    Worker 0: Train Loss: 3.0098, Train Accuracy: 25.15%\n",
            "    Worker 1: Train Loss: 2.9715, Train Accuracy: 25.59%\n",
            "    Worker 2: Train Loss: 3.0114, Train Accuracy: 25.56%\n",
            "    Worker 3: Train Loss: 3.0386, Train Accuracy: 24.49%\n",
            "    Worker 4: Train Loss: 2.9823, Train Accuracy: 25.73%\n",
            "    Worker 5: Train Loss: 2.9747, Train Accuracy: 26.36%\n",
            "    Worker 6: Train Loss: 3.0308, Train Accuracy: 24.99%\n",
            "    Worker 7: Train Loss: 2.9981, Train Accuracy: 25.73%\n",
            "Epoch 16/150: Test Loss: 2.9288, Test Accuracy: 28.14%\n",
            "    Worker 0: Train Loss: 2.9853, Train Accuracy: 25.07%\n",
            "    Worker 1: Train Loss: 2.9182, Train Accuracy: 27.27%\n",
            "    Worker 2: Train Loss: 2.9868, Train Accuracy: 26.12%\n",
            "    Worker 3: Train Loss: 2.9690, Train Accuracy: 25.96%\n",
            "    Worker 4: Train Loss: 2.9412, Train Accuracy: 27.55%\n",
            "    Worker 5: Train Loss: 2.9015, Train Accuracy: 28.13%\n",
            "    Worker 6: Train Loss: 2.9536, Train Accuracy: 26.43%\n",
            "    Worker 7: Train Loss: 2.9665, Train Accuracy: 26.17%\n",
            "Epoch 17/150: Test Loss: 2.8817, Test Accuracy: 29.02%\n",
            "    Worker 0: Train Loss: 2.9453, Train Accuracy: 26.04%\n",
            "    Worker 1: Train Loss: 2.9128, Train Accuracy: 26.67%\n",
            "    Worker 2: Train Loss: 2.9044, Train Accuracy: 27.86%\n",
            "    Worker 3: Train Loss: 2.9644, Train Accuracy: 26.06%\n",
            "    Worker 4: Train Loss: 2.8794, Train Accuracy: 28.30%\n",
            "    Worker 5: Train Loss: 2.9220, Train Accuracy: 27.63%\n",
            "    Worker 6: Train Loss: 2.9653, Train Accuracy: 27.20%\n",
            "    Worker 7: Train Loss: 2.9259, Train Accuracy: 27.30%\n",
            "Epoch 18/150: Test Loss: 2.8170, Test Accuracy: 30.12%\n",
            "    Worker 0: Train Loss: 2.8353, Train Accuracy: 29.68%\n",
            "    Worker 1: Train Loss: 2.8444, Train Accuracy: 28.44%\n",
            "    Worker 2: Train Loss: 2.8318, Train Accuracy: 29.21%\n",
            "    Worker 3: Train Loss: 2.8682, Train Accuracy: 27.49%\n",
            "    Worker 4: Train Loss: 2.8227, Train Accuracy: 28.60%\n",
            "    Worker 5: Train Loss: 2.8190, Train Accuracy: 29.05%\n",
            "    Worker 6: Train Loss: 2.8421, Train Accuracy: 28.61%\n",
            "    Worker 7: Train Loss: 2.8158, Train Accuracy: 29.55%\n",
            "Epoch 19/150: Test Loss: 2.7833, Test Accuracy: 30.45%\n",
            "    Worker 0: Train Loss: 2.7546, Train Accuracy: 30.39%\n",
            "    Worker 1: Train Loss: 2.7530, Train Accuracy: 30.92%\n",
            "    Worker 2: Train Loss: 2.7745, Train Accuracy: 30.68%\n",
            "    Worker 3: Train Loss: 2.7840, Train Accuracy: 29.18%\n",
            "    Worker 4: Train Loss: 2.7621, Train Accuracy: 30.01%\n",
            "    Worker 5: Train Loss: 2.7388, Train Accuracy: 31.44%\n",
            "    Worker 6: Train Loss: 2.7761, Train Accuracy: 30.21%\n",
            "    Worker 7: Train Loss: 2.7582, Train Accuracy: 30.43%\n"
          ]
        }
      ],
      "source": [
        "%run localSGD.py --k 8 --j 4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}