{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXtKbnOAeMli",
        "outputId": "5a61189d-6f9d-4653-ca42-dfcc334296ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting localSGD.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localSGD.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "# Alpha slow learning rate a=1 best\n",
        "# Beta slow momentum\n",
        "global_iteration_per_epoch = 782 # to be divided by K*J\n",
        "num_epochs = 150\n",
        "batch_size = 64\n",
        "base_learning_rate = 1e-3\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "#warmup_epochs = 5\n",
        "\n",
        "alpha = 1.0  # Slow learning rate scaling factor\n",
        "beta = 0.5  # Slow momentum factor\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    \"\"\"Compute the mean and std of CIFAR-100 dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: A dataset derived from `torch.utils.data.Dataset`,\n",
        "                 such as `cifar100_training_dataset` or `cifar100_test_dataset`.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing (mean, std) for the entire dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "global_iteration_per_epoch = math.ceil(global_iteration_per_epoch / (K * J)) * (K * J)\n",
        "global_iteration_per_epoch = global_iteration_per_epoch // (K * J)\n",
        "print(f\"Global iteration per epoch: {global_iteration_per_epoch}\")\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "# After creating the DataLoader for each worker, create an iterator\n",
        "shard_iterators = [iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True)) for k in range(K)]\n",
        "\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate for each k: {learning_rate:.5f}\")\n",
        "#lr_warmup=learning_rate\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for net in nets[1:]:  # Skip nets[0] because it's the reference model\n",
        "        for param_target, param_source in zip(net.parameters(), nets[0].parameters()):\n",
        "            param_target.data.copy_(param_source.data)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs) for optimizer in optimizers]\n",
        "\n",
        "u_buffer = [torch.zeros_like(param) for param in nets[0].parameters()]  # Momentum buffer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize storage for train losses and accuracies for each worker\n",
        "train_losses_per_worker = [[] for _ in range(K)]\n",
        "train_accuracies_per_worker = [[] for _ in range(K)]\n",
        "\n",
        "# Initialize test losses and accuracies for test set\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "    #if epoch < warmup_epochs: #lr for buffer update during warmup\n",
        "            #w_factor = (epoch + 1) / warmup_epochs\n",
        "            #lr_warmup = w_factor * learning_rate\n",
        "    train_loss_for_iteration = [0.0 for _ in range(K)]\n",
        "    correct_train_for_iteration = [0 for _ in range(K)]\n",
        "    total_train_for_iteration = [0 for _ in range(K)]\n",
        "    for iteration in range(global_iteration_per_epoch): #outer loop\n",
        "\n",
        "        #x(t,0)\n",
        "        iter_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "        for param, g_param in zip(iter_params, nets[0].parameters()):\n",
        "                    param.data.copy_(g_param)\n",
        "        # Sequentially train each worker to simulate a distributed environment\n",
        "        for k in range(K):\n",
        "\n",
        "            nets[k].train()\n",
        "            correct_train, total_train, train_loss = 0, 0, 0.0\n",
        "\n",
        "            # Perform J local step before synchronization\n",
        "            for j in range(J): #inner loop\n",
        "                      try:\n",
        "                          # Get a batch from the current worker's shard using next()\n",
        "                          images, labels = next(shard_iterators[k])\n",
        "                          images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "                      except StopIteration:\n",
        "                          # If the iterator has exhausted the data, restart the iterator\n",
        "                          shard_iterators[k] = iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True))\n",
        "                          images, labels = next(shard_iterators[k]) # Resume from the next batch\n",
        "                          images, labels = images.to(device), labels.to(device)\n",
        "                          # Perform optimization step\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_loss_for_iteration[k] += train_loss\n",
        "            correct_train_for_iteration[k] += correct_train\n",
        "            total_train_for_iteration[k] += total_train\n",
        "\n",
        "\n",
        "         # Synchronization (SLOWMO Algothm 1 - paper 21)\n",
        "        with torch.no_grad():\n",
        "            global_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "            #x(t, tau)\n",
        "            for net in nets:\n",
        "                for global_param, local_param in zip(global_params, net.parameters()):\n",
        "                    global_param += local_param\n",
        "            for global_param in global_params:\n",
        "                global_param /= K\n",
        "\n",
        "            for param, g_param, u_param in zip(iter_params, global_params, u_buffer):\n",
        "                #if epoch < warmup_epochs:\n",
        "                  #  u_param.mul_(beta).add_((param - g_param) / lr_warmup)\n",
        "                 #   g_param.sub_(alpha * lr_warmup * u_param)\n",
        "\n",
        "                #else: #Sync after warmup\n",
        "                    #print(f\"{optimizers[0].param_groups[0]['lr']}  {optimizers[1].param_groups[0]['lr']}\")\n",
        "                    lr_sched = optimizers[0].param_groups[0]['lr'] #optimizer lr\n",
        "                    u_param.mul_(beta).add_((param - g_param) / lr_sched)\n",
        "                    #g_param.sub_(alpha * lr_sched * u_param)\n",
        "                    g_param.copy_(param - alpha * lr_sched * u_param)\n",
        "\n",
        "            for net in nets:\n",
        "                for param, g_param in zip(net.parameters(), global_params):\n",
        "                    param.data.copy_(g_param)\n",
        "\n",
        "    # Update lr for each scheduler\n",
        "    for scheduler in schedulers:\n",
        "        #if epoch >= warmup_epochs:\n",
        "            scheduler.step()  # Trigger cosine decay\n",
        "        #else:\n",
        "         #   warmup_factor = (epoch + 1) / warmup_epochs\n",
        "          #  lr = warmup_factor * learning_rate\n",
        "           # for optimizer in optimizers:\n",
        "            #    for param_group in optimizer.param_groups:\n",
        "             #       param_group['lr'] = lr\n",
        "        # print(f\"Learning rate at epoch {epoch + 1}: {scheduler.get_lr()[0]:.5f}\")\n",
        "\n",
        "    # Final Evaluation on Test Set\n",
        "    global_model = nets[0] # all models are synchronized\n",
        "    global_model.eval()\n",
        "    # Test\n",
        "    correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = 100. * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    for k in range(K):\n",
        "        # Store train loss and accuracy for this worker\n",
        "        train_loss_for_iteration[k] /= total_train_for_iteration[k]\n",
        "        train_losses_per_worker[k].append(train_loss_for_iteration[k])\n",
        "        tot_accuracy = 100. * correct_train_for_iteration[k] / total_train_for_iteration[k]\n",
        "        train_accuracies_per_worker[k].append(tot_accuracy)\n",
        "        print(f\"    Worker {k}: Train Loss: {train_losses_per_worker[k][-1]:.4f}, Train Accuracy: {train_accuracies_per_worker[k][-1]:.2f}%\")\n",
        "\n",
        "# Plot results\n",
        "# Plot Train Loss for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_losses_per_worker[k], label=f'Worker {k} Train Loss')\n",
        "plt.title('Train Loss per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracy for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_accuracies_per_worker[k], label=f'Worker {k} Train Accuracy')\n",
        "plt.title('Train Accuracy per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run localSGD.py --k 3 --j 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LaIphbKpsV-i",
        "outputId": "db1ccc15-375a-45e5-cdc9-ba86b2033b16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with K=3, J=8\n",
            "Global iteration per epoch: 33\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Base learning rate for each k: 0.00300\n",
            "Epoch 1/150: Test Loss: 4.3509, Test Accuracy: 4.09%\n",
            "    Worker 0: Train Loss: 4.5585, Train Accuracy: 2.00%\n",
            "    Worker 1: Train Loss: 4.5557, Train Accuracy: 2.10%\n",
            "    Worker 2: Train Loss: 4.5603, Train Accuracy: 2.03%\n",
            "Epoch 2/150: Test Loss: 3.9396, Test Accuracy: 9.46%\n",
            "    Worker 0: Train Loss: 4.1320, Train Accuracy: 6.93%\n",
            "    Worker 1: Train Loss: 4.1212, Train Accuracy: 7.20%\n",
            "    Worker 2: Train Loss: 4.1312, Train Accuracy: 6.85%\n",
            "Epoch 3/150: Test Loss: 3.7427, Test Accuracy: 12.92%\n",
            "    Worker 0: Train Loss: 3.8813, Train Accuracy: 10.38%\n",
            "    Worker 1: Train Loss: 3.8568, Train Accuracy: 11.08%\n",
            "    Worker 2: Train Loss: 3.8705, Train Accuracy: 10.42%\n",
            "Epoch 4/150: Test Loss: 3.5505, Test Accuracy: 15.96%\n",
            "    Worker 0: Train Loss: 3.6956, Train Accuracy: 13.04%\n",
            "    Worker 1: Train Loss: 3.6577, Train Accuracy: 13.95%\n",
            "    Worker 2: Train Loss: 3.6830, Train Accuracy: 13.58%\n",
            "Epoch 5/150: Test Loss: 3.3762, Test Accuracy: 19.55%\n",
            "    Worker 0: Train Loss: 3.5027, Train Accuracy: 16.28%\n",
            "    Worker 1: Train Loss: 3.4876, Train Accuracy: 17.00%\n",
            "    Worker 2: Train Loss: 3.4948, Train Accuracy: 17.01%\n",
            "Epoch 6/150: Test Loss: 3.2387, Test Accuracy: 21.43%\n",
            "    Worker 0: Train Loss: 3.3707, Train Accuracy: 18.44%\n",
            "    Worker 1: Train Loss: 3.3481, Train Accuracy: 19.05%\n",
            "    Worker 2: Train Loss: 3.3699, Train Accuracy: 18.61%\n",
            "Epoch 7/150: Test Loss: 3.1411, Test Accuracy: 23.38%\n",
            "    Worker 0: Train Loss: 3.2338, Train Accuracy: 21.43%\n",
            "    Worker 1: Train Loss: 3.2329, Train Accuracy: 21.22%\n",
            "    Worker 2: Train Loss: 3.2399, Train Accuracy: 21.09%\n",
            "Epoch 8/150: Test Loss: 3.0394, Test Accuracy: 25.53%\n",
            "    Worker 0: Train Loss: 3.1276, Train Accuracy: 22.59%\n",
            "    Worker 1: Train Loss: 3.0966, Train Accuracy: 23.61%\n",
            "    Worker 2: Train Loss: 3.1226, Train Accuracy: 23.12%\n",
            "Epoch 9/150: Test Loss: 2.8916, Test Accuracy: 28.18%\n",
            "    Worker 0: Train Loss: 3.0177, Train Accuracy: 25.27%\n",
            "    Worker 1: Train Loss: 3.0011, Train Accuracy: 25.10%\n",
            "    Worker 2: Train Loss: 3.0106, Train Accuracy: 25.54%\n",
            "Epoch 10/150: Test Loss: 2.8090, Test Accuracy: 29.06%\n",
            "    Worker 0: Train Loss: 2.9039, Train Accuracy: 27.32%\n",
            "    Worker 1: Train Loss: 2.8959, Train Accuracy: 27.67%\n",
            "    Worker 2: Train Loss: 2.9270, Train Accuracy: 27.04%\n",
            "Epoch 11/150: Test Loss: 2.7175, Test Accuracy: 31.66%\n",
            "    Worker 0: Train Loss: 2.8157, Train Accuracy: 29.04%\n",
            "    Worker 1: Train Loss: 2.8019, Train Accuracy: 29.02%\n",
            "    Worker 2: Train Loss: 2.8361, Train Accuracy: 28.46%\n",
            "Epoch 12/150: Test Loss: 2.6522, Test Accuracy: 32.34%\n",
            "    Worker 0: Train Loss: 2.7498, Train Accuracy: 30.07%\n",
            "    Worker 1: Train Loss: 2.7401, Train Accuracy: 30.43%\n",
            "    Worker 2: Train Loss: 2.7551, Train Accuracy: 30.40%\n",
            "Epoch 13/150: Test Loss: 2.5753, Test Accuracy: 33.89%\n",
            "    Worker 0: Train Loss: 2.6604, Train Accuracy: 31.86%\n",
            "    Worker 1: Train Loss: 2.6642, Train Accuracy: 32.29%\n",
            "    Worker 2: Train Loss: 2.6652, Train Accuracy: 32.06%\n",
            "Epoch 14/150: Test Loss: 2.5656, Test Accuracy: 34.26%\n",
            "    Worker 0: Train Loss: 2.6199, Train Accuracy: 32.77%\n",
            "    Worker 1: Train Loss: 2.5981, Train Accuracy: 33.40%\n",
            "    Worker 2: Train Loss: 2.6210, Train Accuracy: 32.95%\n",
            "Epoch 15/150: Test Loss: 2.4855, Test Accuracy: 36.19%\n",
            "    Worker 0: Train Loss: 2.5569, Train Accuracy: 33.88%\n",
            "    Worker 1: Train Loss: 2.5549, Train Accuracy: 34.16%\n",
            "    Worker 2: Train Loss: 2.5579, Train Accuracy: 34.26%\n",
            "Epoch 16/150: Test Loss: 2.4326, Test Accuracy: 37.36%\n",
            "    Worker 0: Train Loss: 2.4904, Train Accuracy: 35.47%\n",
            "    Worker 1: Train Loss: 2.4812, Train Accuracy: 35.57%\n",
            "    Worker 2: Train Loss: 2.4977, Train Accuracy: 35.51%\n",
            "Epoch 17/150: Test Loss: 2.4020, Test Accuracy: 38.36%\n",
            "    Worker 0: Train Loss: 2.4514, Train Accuracy: 36.29%\n",
            "    Worker 1: Train Loss: 2.4299, Train Accuracy: 37.23%\n",
            "    Worker 2: Train Loss: 2.4513, Train Accuracy: 36.68%\n",
            "Epoch 18/150: Test Loss: 2.3757, Test Accuracy: 38.22%\n",
            "    Worker 0: Train Loss: 2.4131, Train Accuracy: 37.26%\n",
            "    Worker 1: Train Loss: 2.4008, Train Accuracy: 37.38%\n",
            "    Worker 2: Train Loss: 2.4014, Train Accuracy: 37.78%\n",
            "Epoch 19/150: Test Loss: 2.3463, Test Accuracy: 39.51%\n",
            "    Worker 0: Train Loss: 2.3561, Train Accuracy: 38.03%\n",
            "    Worker 1: Train Loss: 2.3464, Train Accuracy: 38.95%\n",
            "    Worker 2: Train Loss: 2.3749, Train Accuracy: 38.23%\n",
            "Epoch 20/150: Test Loss: 2.3216, Test Accuracy: 39.87%\n",
            "    Worker 0: Train Loss: 2.3043, Train Accuracy: 39.26%\n",
            "    Worker 1: Train Loss: 2.3021, Train Accuracy: 39.83%\n",
            "    Worker 2: Train Loss: 2.3092, Train Accuracy: 39.74%\n",
            "Epoch 21/150: Test Loss: 2.2919, Test Accuracy: 40.38%\n",
            "    Worker 0: Train Loss: 2.2670, Train Accuracy: 40.37%\n",
            "    Worker 1: Train Loss: 2.2611, Train Accuracy: 40.34%\n",
            "    Worker 2: Train Loss: 2.2874, Train Accuracy: 40.05%\n",
            "Epoch 22/150: Test Loss: 2.3137, Test Accuracy: 39.95%\n",
            "    Worker 0: Train Loss: 2.2432, Train Accuracy: 41.04%\n",
            "    Worker 1: Train Loss: 2.2427, Train Accuracy: 40.41%\n",
            "    Worker 2: Train Loss: 2.2441, Train Accuracy: 41.08%\n",
            "Epoch 23/150: Test Loss: 2.2514, Test Accuracy: 40.81%\n",
            "    Worker 0: Train Loss: 2.1743, Train Accuracy: 42.19%\n",
            "    Worker 1: Train Loss: 2.1709, Train Accuracy: 42.22%\n",
            "    Worker 2: Train Loss: 2.1983, Train Accuracy: 41.97%\n",
            "Epoch 24/150: Test Loss: 2.2232, Test Accuracy: 42.44%\n",
            "    Worker 0: Train Loss: 2.1494, Train Accuracy: 43.12%\n",
            "    Worker 1: Train Loss: 2.1544, Train Accuracy: 42.73%\n",
            "    Worker 2: Train Loss: 2.1940, Train Accuracy: 42.18%\n",
            "Epoch 25/150: Test Loss: 2.2065, Test Accuracy: 42.86%\n",
            "    Worker 0: Train Loss: 2.1324, Train Accuracy: 42.99%\n",
            "    Worker 1: Train Loss: 2.1167, Train Accuracy: 43.69%\n",
            "    Worker 2: Train Loss: 2.1092, Train Accuracy: 43.37%\n",
            "Epoch 26/150: Test Loss: 2.1740, Test Accuracy: 43.06%\n",
            "    Worker 0: Train Loss: 2.0781, Train Accuracy: 44.09%\n",
            "    Worker 1: Train Loss: 2.0947, Train Accuracy: 43.95%\n",
            "    Worker 2: Train Loss: 2.1255, Train Accuracy: 43.55%\n",
            "Epoch 27/150: Test Loss: 2.1509, Test Accuracy: 44.13%\n",
            "    Worker 0: Train Loss: 2.0611, Train Accuracy: 44.89%\n",
            "    Worker 1: Train Loss: 2.0538, Train Accuracy: 44.61%\n",
            "    Worker 2: Train Loss: 2.0829, Train Accuracy: 44.64%\n",
            "Epoch 28/150: Test Loss: 2.1651, Test Accuracy: 43.01%\n",
            "    Worker 0: Train Loss: 2.0128, Train Accuracy: 45.74%\n",
            "    Worker 1: Train Loss: 2.0212, Train Accuracy: 45.56%\n",
            "    Worker 2: Train Loss: 2.0400, Train Accuracy: 45.66%\n",
            "Epoch 29/150: Test Loss: 2.1631, Test Accuracy: 43.68%\n",
            "    Worker 0: Train Loss: 2.0089, Train Accuracy: 45.99%\n",
            "    Worker 1: Train Loss: 2.0191, Train Accuracy: 45.90%\n",
            "    Worker 2: Train Loss: 2.0288, Train Accuracy: 45.98%\n",
            "Epoch 30/150: Test Loss: 2.1262, Test Accuracy: 44.48%\n",
            "    Worker 0: Train Loss: 1.9802, Train Accuracy: 46.41%\n",
            "    Worker 1: Train Loss: 1.9859, Train Accuracy: 46.49%\n",
            "    Worker 2: Train Loss: 1.9957, Train Accuracy: 46.77%\n",
            "Epoch 31/150: Test Loss: 2.1104, Test Accuracy: 45.45%\n",
            "    Worker 0: Train Loss: 1.9383, Train Accuracy: 47.46%\n",
            "    Worker 1: Train Loss: 1.9562, Train Accuracy: 47.36%\n",
            "    Worker 2: Train Loss: 1.9504, Train Accuracy: 47.39%\n",
            "Epoch 32/150: Test Loss: 2.0871, Test Accuracy: 45.67%\n",
            "    Worker 0: Train Loss: 1.9308, Train Accuracy: 47.85%\n",
            "    Worker 1: Train Loss: 1.9027, Train Accuracy: 48.53%\n",
            "    Worker 2: Train Loss: 1.9229, Train Accuracy: 47.95%\n",
            "Epoch 33/150: Test Loss: 2.0645, Test Accuracy: 46.32%\n",
            "    Worker 0: Train Loss: 1.8772, Train Accuracy: 48.70%\n",
            "    Worker 1: Train Loss: 1.8663, Train Accuracy: 49.47%\n",
            "    Worker 2: Train Loss: 1.9017, Train Accuracy: 48.71%\n",
            "Epoch 34/150: Test Loss: 2.0849, Test Accuracy: 45.28%\n",
            "    Worker 0: Train Loss: 1.8640, Train Accuracy: 49.19%\n",
            "    Worker 1: Train Loss: 1.8702, Train Accuracy: 49.20%\n",
            "    Worker 2: Train Loss: 1.8808, Train Accuracy: 48.78%\n",
            "Epoch 35/150: Test Loss: 2.0561, Test Accuracy: 46.23%\n",
            "    Worker 0: Train Loss: 1.8298, Train Accuracy: 49.83%\n",
            "    Worker 1: Train Loss: 1.8440, Train Accuracy: 49.82%\n",
            "    Worker 2: Train Loss: 1.8516, Train Accuracy: 49.53%\n",
            "Epoch 36/150: Test Loss: 2.0400, Test Accuracy: 46.45%\n",
            "    Worker 0: Train Loss: 1.8129, Train Accuracy: 50.30%\n",
            "    Worker 1: Train Loss: 1.8093, Train Accuracy: 50.75%\n",
            "    Worker 2: Train Loss: 1.8276, Train Accuracy: 50.20%\n",
            "Epoch 37/150: Test Loss: 2.0712, Test Accuracy: 45.60%\n",
            "    Worker 0: Train Loss: 1.7867, Train Accuracy: 50.59%\n",
            "    Worker 1: Train Loss: 1.7880, Train Accuracy: 50.65%\n",
            "    Worker 2: Train Loss: 1.8118, Train Accuracy: 50.81%\n",
            "Epoch 38/150: Test Loss: 2.0344, Test Accuracy: 46.39%\n",
            "    Worker 0: Train Loss: 1.7662, Train Accuracy: 51.37%\n",
            "    Worker 1: Train Loss: 1.7735, Train Accuracy: 51.55%\n",
            "    Worker 2: Train Loss: 1.7895, Train Accuracy: 50.91%\n",
            "Epoch 39/150: Test Loss: 2.0732, Test Accuracy: 46.16%\n",
            "    Worker 0: Train Loss: 1.7510, Train Accuracy: 52.07%\n",
            "    Worker 1: Train Loss: 1.7558, Train Accuracy: 51.02%\n",
            "    Worker 2: Train Loss: 1.7672, Train Accuracy: 51.71%\n",
            "Epoch 40/150: Test Loss: 2.0228, Test Accuracy: 47.23%\n",
            "    Worker 0: Train Loss: 1.7099, Train Accuracy: 52.70%\n",
            "    Worker 1: Train Loss: 1.7540, Train Accuracy: 51.92%\n",
            "    Worker 2: Train Loss: 1.7348, Train Accuracy: 52.29%\n",
            "Epoch 41/150: Test Loss: 2.0402, Test Accuracy: 46.85%\n",
            "    Worker 0: Train Loss: 1.7022, Train Accuracy: 52.40%\n",
            "    Worker 1: Train Loss: 1.6918, Train Accuracy: 52.93%\n",
            "    Worker 2: Train Loss: 1.7260, Train Accuracy: 52.15%\n",
            "Epoch 42/150: Test Loss: 2.0179, Test Accuracy: 47.71%\n",
            "    Worker 0: Train Loss: 1.6907, Train Accuracy: 53.08%\n",
            "    Worker 1: Train Loss: 1.6912, Train Accuracy: 52.98%\n",
            "    Worker 2: Train Loss: 1.6953, Train Accuracy: 53.42%\n",
            "Epoch 43/150: Test Loss: 1.9913, Test Accuracy: 48.00%\n",
            "    Worker 0: Train Loss: 1.6703, Train Accuracy: 53.17%\n",
            "    Worker 1: Train Loss: 1.6673, Train Accuracy: 53.97%\n",
            "    Worker 2: Train Loss: 1.6822, Train Accuracy: 53.50%\n",
            "Epoch 44/150: Test Loss: 2.0113, Test Accuracy: 47.63%\n",
            "    Worker 0: Train Loss: 1.6524, Train Accuracy: 53.76%\n",
            "    Worker 1: Train Loss: 1.6651, Train Accuracy: 54.07%\n",
            "    Worker 2: Train Loss: 1.6620, Train Accuracy: 54.27%\n",
            "Epoch 45/150: Test Loss: 2.0033, Test Accuracy: 47.49%\n",
            "    Worker 0: Train Loss: 1.6418, Train Accuracy: 53.93%\n",
            "    Worker 1: Train Loss: 1.6335, Train Accuracy: 54.33%\n",
            "    Worker 2: Train Loss: 1.6496, Train Accuracy: 53.75%\n",
            "Epoch 46/150: Test Loss: 1.9724, Test Accuracy: 48.95%\n",
            "    Worker 0: Train Loss: 1.6135, Train Accuracy: 55.10%\n",
            "    Worker 1: Train Loss: 1.6213, Train Accuracy: 54.51%\n",
            "    Worker 2: Train Loss: 1.6308, Train Accuracy: 54.78%\n",
            "Epoch 47/150: Test Loss: 1.9902, Test Accuracy: 48.13%\n",
            "    Worker 0: Train Loss: 1.5870, Train Accuracy: 55.50%\n",
            "    Worker 1: Train Loss: 1.5881, Train Accuracy: 55.79%\n",
            "    Worker 2: Train Loss: 1.6088, Train Accuracy: 55.39%\n",
            "Epoch 48/150: Test Loss: 2.0154, Test Accuracy: 47.84%\n",
            "    Worker 0: Train Loss: 1.5683, Train Accuracy: 55.85%\n",
            "    Worker 1: Train Loss: 1.5727, Train Accuracy: 56.03%\n",
            "    Worker 2: Train Loss: 1.5949, Train Accuracy: 55.54%\n",
            "Epoch 49/150: Test Loss: 1.9693, Test Accuracy: 48.34%\n",
            "    Worker 0: Train Loss: 1.5377, Train Accuracy: 56.73%\n",
            "    Worker 1: Train Loss: 1.5512, Train Accuracy: 56.57%\n",
            "    Worker 2: Train Loss: 1.5775, Train Accuracy: 56.36%\n",
            "Epoch 50/150: Test Loss: 1.9944, Test Accuracy: 48.49%\n",
            "    Worker 0: Train Loss: 1.5707, Train Accuracy: 55.80%\n",
            "    Worker 1: Train Loss: 1.5625, Train Accuracy: 56.19%\n",
            "    Worker 2: Train Loss: 1.5517, Train Accuracy: 56.38%\n",
            "Epoch 51/150: Test Loss: 1.9659, Test Accuracy: 49.26%\n",
            "    Worker 0: Train Loss: 1.5121, Train Accuracy: 57.65%\n",
            "    Worker 1: Train Loss: 1.5391, Train Accuracy: 56.81%\n",
            "    Worker 2: Train Loss: 1.5431, Train Accuracy: 56.97%\n",
            "Epoch 52/150: Test Loss: 2.0418, Test Accuracy: 47.66%\n",
            "    Worker 0: Train Loss: 1.5103, Train Accuracy: 57.85%\n",
            "    Worker 1: Train Loss: 1.5269, Train Accuracy: 57.00%\n",
            "    Worker 2: Train Loss: 1.5332, Train Accuracy: 56.71%\n",
            "Epoch 53/150: Test Loss: 2.0020, Test Accuracy: 48.42%\n",
            "    Worker 0: Train Loss: 1.5266, Train Accuracy: 57.05%\n",
            "    Worker 1: Train Loss: 1.5072, Train Accuracy: 57.72%\n",
            "    Worker 2: Train Loss: 1.5209, Train Accuracy: 57.42%\n",
            "Epoch 54/150: Test Loss: 1.9676, Test Accuracy: 49.56%\n",
            "    Worker 0: Train Loss: 1.4842, Train Accuracy: 58.21%\n",
            "    Worker 1: Train Loss: 1.4893, Train Accuracy: 57.98%\n",
            "    Worker 2: Train Loss: 1.5274, Train Accuracy: 57.31%\n",
            "Epoch 55/150: Test Loss: 1.9585, Test Accuracy: 49.74%\n",
            "    Worker 0: Train Loss: 1.4765, Train Accuracy: 58.65%\n",
            "    Worker 1: Train Loss: 1.4597, Train Accuracy: 58.48%\n",
            "    Worker 2: Train Loss: 1.4640, Train Accuracy: 58.20%\n",
            "Epoch 56/150: Test Loss: 1.9668, Test Accuracy: 49.11%\n",
            "    Worker 0: Train Loss: 1.4551, Train Accuracy: 58.38%\n",
            "    Worker 1: Train Loss: 1.4567, Train Accuracy: 58.99%\n",
            "    Worker 2: Train Loss: 1.4869, Train Accuracy: 58.06%\n",
            "Epoch 57/150: Test Loss: 1.9639, Test Accuracy: 49.42%\n",
            "    Worker 0: Train Loss: 1.4241, Train Accuracy: 59.63%\n",
            "    Worker 1: Train Loss: 1.4177, Train Accuracy: 59.80%\n",
            "    Worker 2: Train Loss: 1.4321, Train Accuracy: 59.67%\n",
            "Epoch 58/150: Test Loss: 1.9723, Test Accuracy: 49.49%\n",
            "    Worker 0: Train Loss: 1.4240, Train Accuracy: 59.63%\n",
            "    Worker 1: Train Loss: 1.4372, Train Accuracy: 59.40%\n",
            "    Worker 2: Train Loss: 1.4362, Train Accuracy: 59.32%\n",
            "Epoch 59/150: Test Loss: 1.9909, Test Accuracy: 49.19%\n",
            "    Worker 0: Train Loss: 1.3959, Train Accuracy: 60.43%\n",
            "    Worker 1: Train Loss: 1.4168, Train Accuracy: 59.55%\n",
            "    Worker 2: Train Loss: 1.4112, Train Accuracy: 59.81%\n",
            "Epoch 60/150: Test Loss: 2.0211, Test Accuracy: 49.21%\n",
            "    Worker 0: Train Loss: 1.3812, Train Accuracy: 60.83%\n",
            "    Worker 1: Train Loss: 1.3987, Train Accuracy: 60.27%\n",
            "    Worker 2: Train Loss: 1.3922, Train Accuracy: 60.32%\n",
            "Epoch 61/150: Test Loss: 2.0107, Test Accuracy: 49.45%\n",
            "    Worker 0: Train Loss: 1.3866, Train Accuracy: 60.25%\n",
            "    Worker 1: Train Loss: 1.4037, Train Accuracy: 59.76%\n",
            "    Worker 2: Train Loss: 1.4040, Train Accuracy: 59.91%\n",
            "Epoch 62/150: Test Loss: 1.9647, Test Accuracy: 49.61%\n",
            "    Worker 0: Train Loss: 1.3834, Train Accuracy: 60.14%\n",
            "    Worker 1: Train Loss: 1.3723, Train Accuracy: 60.90%\n",
            "    Worker 2: Train Loss: 1.3783, Train Accuracy: 60.71%\n",
            "Epoch 63/150: Test Loss: 1.9928, Test Accuracy: 50.55%\n",
            "    Worker 0: Train Loss: 1.3489, Train Accuracy: 61.02%\n",
            "    Worker 1: Train Loss: 1.3544, Train Accuracy: 61.53%\n",
            "    Worker 2: Train Loss: 1.3692, Train Accuracy: 61.19%\n",
            "Epoch 64/150: Test Loss: 1.9590, Test Accuracy: 50.55%\n",
            "    Worker 0: Train Loss: 1.3438, Train Accuracy: 61.42%\n",
            "    Worker 1: Train Loss: 1.3416, Train Accuracy: 61.93%\n",
            "    Worker 2: Train Loss: 1.3440, Train Accuracy: 61.70%\n",
            "Epoch 65/150: Test Loss: 1.9696, Test Accuracy: 50.12%\n",
            "    Worker 0: Train Loss: 1.3188, Train Accuracy: 62.33%\n",
            "    Worker 1: Train Loss: 1.3096, Train Accuracy: 62.69%\n",
            "    Worker 2: Train Loss: 1.3280, Train Accuracy: 62.11%\n",
            "Epoch 66/150: Test Loss: 1.9724, Test Accuracy: 49.74%\n",
            "    Worker 0: Train Loss: 1.3364, Train Accuracy: 61.59%\n",
            "    Worker 1: Train Loss: 1.3252, Train Accuracy: 62.07%\n",
            "    Worker 2: Train Loss: 1.3253, Train Accuracy: 62.50%\n",
            "Epoch 67/150: Test Loss: 2.0159, Test Accuracy: 49.71%\n",
            "    Worker 0: Train Loss: 1.2999, Train Accuracy: 62.56%\n",
            "    Worker 1: Train Loss: 1.2926, Train Accuracy: 62.89%\n",
            "    Worker 2: Train Loss: 1.2926, Train Accuracy: 63.20%\n",
            "Epoch 68/150: Test Loss: 1.9777, Test Accuracy: 50.54%\n",
            "    Worker 0: Train Loss: 1.2698, Train Accuracy: 63.51%\n",
            "    Worker 1: Train Loss: 1.2764, Train Accuracy: 63.27%\n",
            "    Worker 2: Train Loss: 1.2865, Train Accuracy: 62.74%\n",
            "Epoch 69/150: Test Loss: 1.9859, Test Accuracy: 50.22%\n",
            "    Worker 0: Train Loss: 1.2864, Train Accuracy: 63.00%\n",
            "    Worker 1: Train Loss: 1.2920, Train Accuracy: 62.74%\n",
            "    Worker 2: Train Loss: 1.2962, Train Accuracy: 62.44%\n",
            "Epoch 70/150: Test Loss: 2.0075, Test Accuracy: 50.09%\n",
            "    Worker 0: Train Loss: 1.2450, Train Accuracy: 63.69%\n",
            "    Worker 1: Train Loss: 1.2451, Train Accuracy: 64.25%\n",
            "    Worker 2: Train Loss: 1.2717, Train Accuracy: 63.48%\n",
            "Epoch 71/150: Test Loss: 1.9972, Test Accuracy: 49.92%\n",
            "    Worker 0: Train Loss: 1.2433, Train Accuracy: 63.97%\n",
            "    Worker 1: Train Loss: 1.2572, Train Accuracy: 64.40%\n",
            "    Worker 2: Train Loss: 1.2540, Train Accuracy: 63.90%\n",
            "Epoch 72/150: Test Loss: 1.9907, Test Accuracy: 50.37%\n",
            "    Worker 0: Train Loss: 1.2405, Train Accuracy: 63.96%\n",
            "    Worker 1: Train Loss: 1.2430, Train Accuracy: 64.40%\n",
            "    Worker 2: Train Loss: 1.2530, Train Accuracy: 64.12%\n",
            "Epoch 73/150: Test Loss: 1.9823, Test Accuracy: 50.55%\n",
            "    Worker 0: Train Loss: 1.2163, Train Accuracy: 65.00%\n",
            "    Worker 1: Train Loss: 1.2254, Train Accuracy: 64.84%\n",
            "    Worker 2: Train Loss: 1.2255, Train Accuracy: 64.63%\n",
            "Epoch 74/150: Test Loss: 2.0013, Test Accuracy: 49.87%\n",
            "    Worker 0: Train Loss: 1.1863, Train Accuracy: 65.36%\n",
            "    Worker 1: Train Loss: 1.2085, Train Accuracy: 65.18%\n",
            "    Worker 2: Train Loss: 1.2227, Train Accuracy: 64.24%\n",
            "Epoch 75/150: Test Loss: 2.0173, Test Accuracy: 50.70%\n",
            "    Worker 0: Train Loss: 1.2161, Train Accuracy: 64.81%\n",
            "    Worker 1: Train Loss: 1.1984, Train Accuracy: 65.61%\n",
            "    Worker 2: Train Loss: 1.1917, Train Accuracy: 65.69%\n",
            "Epoch 76/150: Test Loss: 1.9791, Test Accuracy: 50.78%\n",
            "    Worker 0: Train Loss: 1.1684, Train Accuracy: 65.52%\n",
            "    Worker 1: Train Loss: 1.1959, Train Accuracy: 65.55%\n",
            "    Worker 2: Train Loss: 1.1953, Train Accuracy: 65.26%\n",
            "Epoch 77/150: Test Loss: 1.9707, Test Accuracy: 50.42%\n",
            "    Worker 0: Train Loss: 1.1712, Train Accuracy: 66.07%\n",
            "    Worker 1: Train Loss: 1.1765, Train Accuracy: 66.16%\n",
            "    Worker 2: Train Loss: 1.1640, Train Accuracy: 66.47%\n",
            "Epoch 78/150: Test Loss: 2.0182, Test Accuracy: 50.57%\n",
            "    Worker 0: Train Loss: 1.1616, Train Accuracy: 65.77%\n",
            "    Worker 1: Train Loss: 1.1680, Train Accuracy: 66.11%\n",
            "    Worker 2: Train Loss: 1.1712, Train Accuracy: 65.89%\n",
            "Epoch 79/150: Test Loss: 1.9926, Test Accuracy: 51.04%\n",
            "    Worker 0: Train Loss: 1.1629, Train Accuracy: 65.86%\n",
            "    Worker 1: Train Loss: 1.1581, Train Accuracy: 66.64%\n",
            "    Worker 2: Train Loss: 1.1616, Train Accuracy: 66.37%\n",
            "Epoch 80/150: Test Loss: 1.9896, Test Accuracy: 51.01%\n",
            "    Worker 0: Train Loss: 1.1346, Train Accuracy: 66.35%\n",
            "    Worker 1: Train Loss: 1.1423, Train Accuracy: 66.66%\n",
            "    Worker 2: Train Loss: 1.1546, Train Accuracy: 66.19%\n",
            "Epoch 81/150: Test Loss: 2.0184, Test Accuracy: 50.94%\n",
            "    Worker 0: Train Loss: 1.1424, Train Accuracy: 66.71%\n",
            "    Worker 1: Train Loss: 1.1475, Train Accuracy: 66.83%\n",
            "    Worker 2: Train Loss: 1.1345, Train Accuracy: 66.83%\n",
            "Epoch 82/150: Test Loss: 2.0201, Test Accuracy: 50.64%\n",
            "    Worker 0: Train Loss: 1.1206, Train Accuracy: 67.20%\n",
            "    Worker 1: Train Loss: 1.1236, Train Accuracy: 67.17%\n",
            "    Worker 2: Train Loss: 1.1244, Train Accuracy: 67.27%\n",
            "Epoch 83/150: Test Loss: 2.0285, Test Accuracy: 50.80%\n",
            "    Worker 0: Train Loss: 1.1131, Train Accuracy: 67.24%\n",
            "    Worker 1: Train Loss: 1.1171, Train Accuracy: 67.73%\n",
            "    Worker 2: Train Loss: 1.1243, Train Accuracy: 67.04%\n",
            "Epoch 84/150: Test Loss: 2.0162, Test Accuracy: 51.28%\n",
            "    Worker 0: Train Loss: 1.0992, Train Accuracy: 67.79%\n",
            "    Worker 1: Train Loss: 1.1020, Train Accuracy: 68.25%\n",
            "    Worker 2: Train Loss: 1.0906, Train Accuracy: 68.02%\n",
            "Epoch 85/150: Test Loss: 2.0208, Test Accuracy: 51.48%\n",
            "    Worker 0: Train Loss: 1.0930, Train Accuracy: 68.04%\n",
            "    Worker 1: Train Loss: 1.0912, Train Accuracy: 68.03%\n",
            "    Worker 2: Train Loss: 1.0878, Train Accuracy: 68.39%\n",
            "Epoch 86/150: Test Loss: 2.0300, Test Accuracy: 50.66%\n",
            "    Worker 0: Train Loss: 1.0637, Train Accuracy: 69.14%\n",
            "    Worker 1: Train Loss: 1.0739, Train Accuracy: 68.80%\n",
            "    Worker 2: Train Loss: 1.0772, Train Accuracy: 68.34%\n",
            "Epoch 87/150: Test Loss: 2.0305, Test Accuracy: 51.22%\n",
            "    Worker 0: Train Loss: 1.0574, Train Accuracy: 68.67%\n",
            "    Worker 1: Train Loss: 1.0660, Train Accuracy: 68.76%\n",
            "    Worker 2: Train Loss: 1.0716, Train Accuracy: 68.72%\n",
            "Epoch 88/150: Test Loss: 2.0722, Test Accuracy: 51.13%\n",
            "    Worker 0: Train Loss: 1.0462, Train Accuracy: 69.32%\n",
            "    Worker 1: Train Loss: 1.0520, Train Accuracy: 69.33%\n",
            "    Worker 2: Train Loss: 1.0520, Train Accuracy: 69.38%\n",
            "Epoch 89/150: Test Loss: 2.0701, Test Accuracy: 50.82%\n",
            "    Worker 0: Train Loss: 1.0477, Train Accuracy: 69.47%\n",
            "    Worker 1: Train Loss: 1.0530, Train Accuracy: 69.01%\n",
            "    Worker 2: Train Loss: 1.0583, Train Accuracy: 68.44%\n",
            "Epoch 90/150: Test Loss: 2.0358, Test Accuracy: 51.42%\n",
            "    Worker 0: Train Loss: 1.0451, Train Accuracy: 69.14%\n",
            "    Worker 1: Train Loss: 1.0404, Train Accuracy: 69.40%\n",
            "    Worker 2: Train Loss: 1.0516, Train Accuracy: 69.11%\n",
            "Epoch 91/150: Test Loss: 2.0524, Test Accuracy: 51.21%\n",
            "    Worker 0: Train Loss: 1.0163, Train Accuracy: 69.83%\n",
            "    Worker 1: Train Loss: 1.0236, Train Accuracy: 70.13%\n",
            "    Worker 2: Train Loss: 1.0311, Train Accuracy: 69.62%\n",
            "Epoch 92/150: Test Loss: 2.0519, Test Accuracy: 51.26%\n",
            "    Worker 0: Train Loss: 1.0219, Train Accuracy: 69.74%\n",
            "    Worker 1: Train Loss: 1.0266, Train Accuracy: 69.92%\n",
            "    Worker 2: Train Loss: 1.0137, Train Accuracy: 69.97%\n",
            "Epoch 93/150: Test Loss: 2.0458, Test Accuracy: 51.39%\n",
            "    Worker 0: Train Loss: 1.0057, Train Accuracy: 70.31%\n",
            "    Worker 1: Train Loss: 1.0142, Train Accuracy: 70.10%\n",
            "    Worker 2: Train Loss: 1.0036, Train Accuracy: 70.35%\n",
            "Epoch 94/150: Test Loss: 2.0699, Test Accuracy: 50.74%\n",
            "    Worker 0: Train Loss: 0.9722, Train Accuracy: 71.36%\n",
            "    Worker 1: Train Loss: 0.9852, Train Accuracy: 70.99%\n",
            "    Worker 2: Train Loss: 0.9904, Train Accuracy: 70.71%\n",
            "Epoch 95/150: Test Loss: 2.0527, Test Accuracy: 51.66%\n",
            "    Worker 0: Train Loss: 0.9767, Train Accuracy: 71.28%\n",
            "    Worker 1: Train Loss: 0.9963, Train Accuracy: 70.41%\n",
            "    Worker 2: Train Loss: 0.9993, Train Accuracy: 70.69%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/localSGD.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    180\u001b[0m                       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                           \u001b[0;31m# Get a batch from the current worker's shard using next()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                           \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                           \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                           \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}