{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Centr_Training_LAMB_LARS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtOx0ESVnO0u",
        "outputId": "a0ff890d-01d1-499b-febb-73ef86ccc10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting large_batch_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile large_batch_training.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model\n",
        "net = LeNet5().to(device)\n",
        "\n",
        "# Optimizer Subclasses\n",
        "class LARS(optim.Optimizer):\n",
        "    def __init__(self, params, lr, momentum=0.9, weight_decay=0, trust_coefficient=0.001):\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coefficient=trust_coefficient)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['momentum_buffer'] = torch.clone(grad).detach()\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad.add_(p.data, alpha=group['weight_decay'])\n",
        "                param_norm = torch.norm(p.data)\n",
        "                grad_norm = torch.norm(grad)\n",
        "                if param_norm > 0 and grad_norm > 0:\n",
        "                    local_lr = group['trust_coefficient'] * param_norm / (grad_norm + 1e-8)\n",
        "                    grad = grad.mul(local_lr)\n",
        "                momentum_buffer = state['momentum_buffer']\n",
        "                momentum_buffer.mul_(group['momentum']).add_(grad)\n",
        "                state['momentum_buffer'] = momentum_buffer\n",
        "                p.data.add_(-group['lr'], momentum_buffer)\n",
        "        return loss\n",
        "\n",
        "class LAMB(optim.Optimizer):\n",
        "    def __init__(self, params, lr, weight_decay=0, betas=(0.9, 0.999), eps=1e-8):\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, betas=betas, eps=eps)\n",
        "        super(LAMB, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                exp_avg = state['exp_avg']\n",
        "                exp_avg_sq = state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                denominator = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                p.data.addcdiv_(-group['lr'], exp_avg, denominator)\n",
        "        return loss\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Large-Batch Optimizers')\n",
        "parser.add_argument('--optimizer', type=str, default='SGDM', choices=['SGDM', 'AdamW', 'LARS', 'LAMB'],\n",
        "                    help='Choose optimizer')\n",
        "parser.add_argument('--batch-size', type=int, default=128, help='Batch size')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='Base learning rate')\n",
        "parser.add_argument('--momentum', type=float, default=0.8, help='Momentum for SGDM and LARS')\n",
        "parser.add_argument('--weight-decay', type=float, default=1e-5, help='Weight decay')\n",
        "parser.add_argument('--b1', type=float, default=0.9, help='Beta1 for AdamW')\n",
        "parser.add_argument('--b2', type=float, default=0.999, help='Beta2 for AdamW')\n",
        "parser.add_argument('--trust-coefficient', type=float, default=0.001, help='Trust coefficient for LARS')\n",
        "parser.add_argument('--lr-scheduler', type=str, default='CosineAnnealingLR', choices=['StepLR', 'CosineAnnealingLR', 'ExponentialLR'],\n",
        "                    help='Learning rate scheduler')\n",
        "parser.add_argument('--lr-step', type=int, default=30, help='Step size for StepLR')\n",
        "parser.add_argument('--lr-tmax', type=int, default=150, help='T max for CosineAnnealingLR')\n",
        "parser.add_argument('--lr-gamma', type=float, default=0.9, help='Gamma for StepLR and ExponentialLR')\n",
        "parser.add_argument('--epochs', type=int, default=150, help='Number of epochs')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Mapping optimizers\n",
        "if args.optimizer == 'SGDM':\n",
        "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "elif args.optimizer == 'AdamW':\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=args.lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)\n",
        "elif args.optimizer == 'LARS':\n",
        "    optimizer = LARS(net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, trust_coefficient=args.trust_coefficient)\n",
        "elif args.optimizer == 'LAMB':\n",
        "    optimizer = LAMB(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "trainset, valset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scheduler\n",
        "if args.lr_scheduler == 'StepLR':\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step, gamma=args.lr_gamma)\n",
        "elif args.lr_scheduler == 'CosineAnnealingLR':\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.lr_tmax)\n",
        "elif args.lr_scheduler == 'ExponentialLR':\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_gamma)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Function\n",
        "def train_model(optimizer, scheduler, model, criterion, trainloader, valloader , testloader, device, epochs, save_checkpoint_interval=10):\n",
        "    train_losses, val_losses, test_losses = [], [], []\n",
        "    train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += predicted.eq(labels).sum().item()\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_accuracy = 100. * correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_val, total_val, val_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += predicted.eq(labels).sum().item()\n",
        "        val_loss /= len(valloader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracy = 100. * correct_val / total_val\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        '''\n",
        "        # Early Stopping\n",
        "        if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          epochs_no_improve = 0\n",
        "        else:\n",
        "          epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve == patience:\n",
        "          print(\"Early stopping!\")\n",
        "          break\n",
        "        '''\n",
        "\n",
        "        # Test\n",
        "        correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += predicted.eq(labels).sum().item()\n",
        "        test_loss /= len(testloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracy = 100. * correct_test / total_test\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Checkpointing\n",
        "        if (epoch + 1) % save_checkpoint_interval == 0:\n",
        "            checkpoint_filename = f'checkpoint_epoch_{epoch + 1}.pth'\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': loss.item(),\n",
        "            }\n",
        "            checkpoint_path = os.path.join('./', checkpoint_filename)\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f'Checkpoint saved at epoch {epoch + 1}: {checkpoint_path}')\n",
        "\n",
        "            # Download the checkpoint\n",
        "            files.download(checkpoint_filename)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "    return train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies\n",
        "\n",
        "# Train the model\n",
        "print(f\"--- Train with {args.optimizer} ---\")\n",
        "train_loss, val_loss, test_loss, train_acc, val_acc, test_acc = train_model(optimizer, scheduler, net, criterion, trainloader, valloader, testloader, device, args.epochs)\n",
        "\n",
        "# Save model\n",
        "torch.save(net.state_dict(), f'net_{args.optimizer}.pth')\n",
        "\n",
        "# Plot results\n",
        "# Plot Training Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Training Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_acc, label='Train Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Validation Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('val_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('val_accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_loss, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_acc, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hEcpkQqYJmP",
        "outputId": "5ccd3f2b-8b9f-439b-af36-b9b125fd4ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with SGDM ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer SGDM --batch-size 128 --lr 0.01 --momentum 0.8 --weight-decay 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2t__56QmDKL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqIzBQKfZFgB",
        "outputId": "38bf7fb7-fac5-4d66-b098-114255ed4a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with AdamW ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer AdamW --batch-size 128 --lr 0.001 --weight-decay 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y8TL9oBZL18",
        "outputId": "54fc2f89-7e7d-461b-fce6-1f0da7310a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with LARS ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer LARS --batch-size 128 --lr 0.01 --momentum 0.8 --weight-decay 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_XY2BkhZOkY",
        "outputId": "8791a547-f93d-4be4-f71d-620d1887000a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with LAMB ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer LAMB --batch-size 128 --lr 0.01 --weight-decay 1e-5"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}